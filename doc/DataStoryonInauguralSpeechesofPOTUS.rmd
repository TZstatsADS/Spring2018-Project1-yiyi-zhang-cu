---
title: "Data Story on Inaugural Speeches of POTUS"
author: "Yiyi Zhang"  
date: "February 2018"
output: html_notebook
---

## Introduction  
What are the most memorable inaugurable address quotes that you can think of off the top of your head? Let me share two of my favorites and see if they are yours too. 

"This great nation will endure as it has endured, will revive and will prosper. So, first of all, let me assert my firm belief that the only thing we have to fear is fear itself."
*- Franklin D. Roosevelt, March 4, 1933.*

"My fellow Americans: Ask not what your country can do for you - ask what you can do for your country. My fellow citizens of the world: Ask not what America will do for you, but what together we can do for the freedom of man."
*- John F. Kennedy, Jan. 20, 1961.*

Although there are a few very memorable, most inaugural addresses have been long forgotten. This note book will give us a chance to take a closer look at the inaugural speeches of the President of the U.S. (POTUS), but from a data scientic perspective. The scope of this analysis includes 58 Inaugural Speeches of 39 POTUS, which are scrapped from [The American Presidentcy Project][1]. 


## Staging {.tabset}
### Overview  

*   **Step 0** - Install Packages and Load Libraries  
*   **Step 1** - Data Harvest: Scrap Speech URLs and Read in Speeches  
*   **Step 2** - Data Processing  
*   **Step 3** - Build Functions  

### Step 0 - Install and Load Libraries  
```{r, message=FALSE, warning=FALSE}
### Install and Load Libraries  
# Packages that will be used for this notebook 
packages.used=c("rvest", "xlsx", "tibble", 
                "tm", "tidytext", "dplyr",
                "wordcloud", "qdap", "syuzhet",
                "beeswarm", "RColorBrewer", "sentimentr",  
                 "gplots", "factoextra", "MASS", 
                 "scales", "RANN", "topicmodels")

# Check packages that need to be installed
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# Install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

# Load libraries  
library("rvest")
library("xlsx")
library("tibble")
library("tm")
library("tidytext")
library("MASS")
library("dplyr")
library("wordcloud")
# You may need to run
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
#library(rJava)
library("qdap")
library("syuzhet")
library("beeswarm")
library("RColorBrewer")
library("gplots")
library("scales")
#library("sentimentr")
library("factoextra")
#library("RANN")
#library("topicmodels")

# Obtain speechFuncs functions from the website listed below 
# 'speechFuncs' includes 'f.speechlinks', 'f.plotsent.len', and 'f.smooth.topic'
# https://github.com/TZstatsADS/ADS_Teaching/blob/master/Tutorials/wk2-TextMining/lib/speechFuncs.R
source("../lib/speechFuncs.R")
# Obtain plotstacked functions from the website listed below  
# 'plotstacked' includes 'plot.stacked'
# https://github.com/TZstatsADS/ADS_Teaching/blob/master/Tutorials/wk2-TextMining/lib/plotstacked.R
#source("../lib/plotstacked.R")
```

*This notebook was prepared with the following environmental settings.*  
```{r}
print(R.version)
```
*Current working director is* 
```{r}
getwd()
```








### Step 1 - Data Harvest  
```{r, message=FALSE, warning=FALSE}
### Data Harvest: Scrap URLs of Inaugural Speeches of POTUS and Read in Speeches 
# Get speeches URLs
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug.urls <- f.speechlinks(main.page) 
# Remove the last line, irrelevant due to error.  
inaug.urls <- inaug.urls[-nrow(inaug.urls),] 
# Format Date
inaug.urls[,1] <- as.Date(inaug.urls[,1], format="%B %e, %Y")

# Read information file provided "../data/InaugurationInfo.xlsx"
file.path <- "../data/InaugurationInfo.xlsx"
inaug.info <- read.xlsx(file.path, sheetIndex = 1, stringsAsFactors = FALSE)

# Add "Date" and "URLs" columns to inaug.info, named as inaug.list
inaug.list <- add_column(inaug.info, Date = inaug.urls[,1], .after = 4)
inaug.list <- add_column(inaug.list, URLs = inaug.urls[,2])
# Update the "Words" variable for Trump's speech
inaug.list$Words[58] <- 1433 
# Make "Words" as numeric variable
inaug.list$Words <- as.numeric(inaug.list$Words) 
# Make name consistent
inaug.list$President[25] <- "Grover Cleveland" 
inaug.list$President[27] <- "Grover Cleveland" 
inaug.list$File[25] <- "GroverCleveland"
inaug.list$File[27] <- "GroverCleveland"

# Read in each speech  
# Add and stage a 'Fulltext' column to  inaug.list
inaug.list$Fulltext <- NA 
# Indentify the folder "../data/InauguralSpeeches/" 
# where to save each speech as  individual '.txt' file 
folder.path <- "../data/InauguralSpeeches/"
# Loop over each row in inaug.list 
for(i in seq(nrow(inaug.list))) {
  # Read speech from the URL and Store it in variable 'text' 
  text <- read_html(inaug.list$URLs[i]) %>% # Load the page
    html_nodes(".displaytext") %>% # Isloate the text
    html_text() # Get the text 
  # Update the 'Fulltext' column of inaug.list with speech text 
  inaug.list$Fulltext[i] <- text 
  # Create file name of each individual speech
  filename <- paste0(folder.path, 
                     "inaug",
                     inaug.list$File[i], "-", 
                     inaug.list$Term[i], ".txt") 
  # Read text into the '.txt' file created
  sink(file = filename) %>% # Open file to write 
  cat(text)  # Write the file
  sink() # Close the file
}

# Save the inaug.list as '.csv' file under folder "../output/"
write.csv(inaug.list, file = "../output/inaug_list.csv")
#summary(inaug.list)
```

### Step 2 - Data Processing  
```{r, message=FALSE, warning=FALSE}
### Data Processing  
# Stage a data.frame 'sentence.list'
sentence.list <- NULL
# Loop over each row in inaug.list 
for(i in 1:nrow(inaug.list)){
  # Split each speech into individual sentences 
  sentences <- sent_detect(inaug.list$Fulltext[i], 
                           endmarks = c("?", ".", "!", "|",";"))
  
  if(length(sentences)>0){
    # Count the number of words in each sentence
    word.count <- word_count(sentences)
    # Calculate the presence of eight different emotions in each sentence
    emotions <- get_nrc_sentiment(sentences)
    #colnames(emotions)=paste0("emo.", colnames(emotions))
    # Scale emotions by word.count
    # In case the word counts are zeros, add 0.01 to word.count 
    emotions <- diag(1/(word.count+0.01))%*%as.matrix(emotions)
    # Update sentence.list 
    # Columns include columns of inaug.list, senteces, word.count, emotions(10), sent.id
    # Rows added while looping over each row in inaug.list 
    sentence.list <- rbind(sentence.list, 
                        cbind(
                          # Columns of inaug.list
                          inaug.list[i,-ncol(inaug.list)],
                          # One column vector of sentences in individual speech 
                          sentences=as.character(sentences), 
                          # One column of word count of corresponding sentence
                          word.count,
                          # 8 emotions columns + 2 valence columns (positive/negative)
                          emotions,
                          # Assign consecutive id to each sentence
                          sent.id=1:length(sentences) 
                          )
    )
  }
}
# Clean up sentence.list
# Some non-sentences exist in raw data due to erroneous extra end-of-sentence marks.
sentence.list <- sentence.list%>%filter(!is.na(word.count)) 
```

### Step 3 - Build Functions {.tabset}  
#### Assign Group and Color
```{r}
# assign.group
# Assign In.var to groups in In.Group.var
assign.group <- function(In.var,In.Group.var){
    for (i in 1:length(In.Group.var)){
      if (In.var %in% unlist(In.Group.var[i])) {return(names(In.Group.var)[i])}
    }
}

# assign.color
# Assign In.File a color correspond to the groups in In.Group
assign.color <- function(In.var,In.Group.var,In.Palette="Set1"){
  col.use <- alpha(brewer.pal(length(In.Group.var),In.Palette),0.9)
    for (i in 1:length(In.Group.var)){
      if (In.var %in% unlist(In.Group.var[i])) {return(col.use[i])}
    }
}
```
#### Length of Speeches
```{r}
# Speech.Length.Bar.Plot
# Create Bar Plot for every File in groups of In.Group
Speech.Length.Bar.Plot <- function(In.list=inaug.list,
                                   In.Group, In.Group.name, 
                                   In.Term=list("First Term"="1",
                                                "Second Term"="2",
                                                "Third Term"="3",
                                                "Fourth Term"="4")){
  var <- In.list%>%
          filter(File%in%unlist(In.Group),
                 Term%in%unlist(In.Term))
  var$File <- factor(var$File)
  var$FileOrdered <- reorder(var$File, var$Words, mean, order=T)
  var$group.var <- unlist(lapply(var$File, assign.group, In.Group))
  var$group.var <- factor(var$group.var, names(In.Group))
  var$col.id <- unlist(lapply(var$File,
                              assign.color, 
                              In.Group))
  
  x <- barplot(var$Words, space = 1,
               col=var$col.id,
               cex.axis = 0.7,
               main = "Number of Words in a Speech" 
              )
  text(cex=0.5, x=x-0.25, y=-1.25, 
       adj=1, srt = 60, xpd = TRUE,
       labels = paste(substr(var$Date,1,4),
                      var$President))
  legend("topright", inset= 0.02, legend =names(In.Group) , 
         text.col = brewer.pal(length(In.Group),"Set1")[1:length(In.Group)], 
         cex = 0.7, box.lty=2 )
}

# Speech.Length.Box.Plot
# Create Box Plot for groups in In.Group
Speech.Length.Box.Plot <- function(In.list=inaug.list, 
                                   In.Group, In.Group.name,
                                   In.Term=c("1","2","3","4")){
  var <- In.list%>%
          filter(File%in%unlist(In.Group),
                 Term%in%In.Term)
  var$File <- factor(var$File)
  var$FileOrdered <- reorder(var$File, var$Words, mean, order=T)
  var$group.var <- unlist(lapply(var$File, assign.group, In.Group))
  var$group.var <- factor(var$group.var, names(In.Group))

  boxplot(var$Words~var$group.var,
          col=alpha(brewer.pal(length(In.Group),"Set1"),0.7)[1:length(In.Group)],
          cex.axis = 0.7,
          main = "Number of Words in a Speech" )
}
```
#### Length of Sentences
```{r}
# Bee.Swarm.Plot
# Create Bee Swarm Plot for every File in groups of In.Group
Bee.Swarm.Plot <- function(In.list=sentence.list, In.Group, In.Group.name){
  var <- In.list%>%filter(File%in%unlist(In.Group))
  var$File <- factor(var$File)
  var$FileOrdered <- reorder(var$File, var$word.count, mean, order=T)
  
  for (i in 1:length(In.Group)){
    if (i==1){
      beeswarm(word.count~FileOrdered, 
             data = var%>%filter(File%in%unlist(In.Group[i])),
             horizontal = TRUE,
             pch=16, col=alpha(brewer.pal(length(In.Group), "Set1")[i], 0.4), 
             cex=0.55, cex.axis=0.55, cex.lab=1,
             spacing=5/nlevels(var$FileOrdered),
             las=2, ylab="", xlab="",
             main="Number of Words in a Sentence")
    }
    else {
      beeswarm(word.count~FileOrdered, 
             data = var%>%filter(File%in%unlist(In.Group[i])),
             add=TRUE,
             horizontal = TRUE,
             pch=16, col=alpha(brewer.pal(length(In.Group), "Set1")[i], 0.4), 
             cex=0.55, cex.axis=0.55, cex.lab=1,
             spacing=5/nlevels(var$FileOrdered)
             )
    }
  }
  
  legend("bottomright", inset= 0.02, legend =names(In.Group) , 
         text.col = brewer.pal(length(In.Group),"Set1")[1:length(In.Group)], 
         cex = 0.7, box.lty=2 )
}


# Bee.Swarm.Plot.Group
# Create Bee Swarm Plot for groups in In.Group
Bee.Swarm.Plot.Group <- function(In.list=sentence.list, In.Group, In.Group.name){
  var <- In.list%>%filter(File%in%unlist(In.Group))
  var$File <- factor(var$File)
  var$FileOrdered <- reorder(var$File, var$word.count, mean, order=T)
  var$group.var <- unlist(lapply(var$File, assign.group, In.Group))
  var$group.var <- factor(var$group.var, names(In.Group))
    
  beeswarm(word.count~group.var, 
             data = var%>%filter(File%in%unlist(In.Group)),
             horizontal = TRUE,
             pch=16, col=alpha(brewer.pal(length(In.Group),
                                          "Set1")[1:length(In.Group)], 
                               0.4), 
             cex=0.55, cex.axis=0.55, cex.lab=1,
             spacing=5/nlevels(var$FileOrdered),
             las=2, ylab="", xlab="",
             main="Number of Words in a Sentence")
  legend("bottomright", inset=0.02, legend =names(In.Group) , 
         text.col = brewer.pal(length(In.Group),"Set1")[1:length(In.Group)], 
         cex = 0.7, box.lty=2 )
  boxplot(var$word.count~var$group.var, horizontal=TRUE, 
          col="#0000ff22", axes=FALSE, add=TRUE)
}
```

```{r}
# Sentence.Searcher.Group
# Genenrates shortest or longest sentences of groups in In.Group
Sentence.Searcher.Group <- function(In.list=sentence.list, 
                                    In.Group, 
                                    In.Term=c("1","2","3","4"),
                                    In.Long=TRUE){
  df <- c(seq(1:5))
  for (i in 1:length(In.Group)){
    var <-   In.list%>%
             filter(File%in%unlist(In.Group[i]), 
                    Term%in%In.Term,
                    word.count>=3)%>%
               arrange(desc(word.count))%>%
               select(sentences)
      if (In.Long==TRUE){
        var <- head(var,5)
      } else {
        var <- tail(var,5)
      } 
      if (nrow(var)!=0){
        df<- cbind(df,var) 
      } else {
        df <- cbind(df,c(rep("SYSTEM: No data available in table",5)))
      }
    }
  df<- df[,-1]
  colnames(df)<- names(In.Group)
  df
}
```

#### Word Cloud
```{r}
# Word.Cloud.Group
# Creat Word Cloud for groups in In.Group
Word.Cloud.Group <- function(In.list=inaug.list, In.Group, In.Group.name){
  for (i in 1:length(In.Group)){
      var <- In.list%>%
             filter(File%in%unlist(In.Group[i]))
      
      
      docs <- Corpus(VectorSource(var$Fulltext))
      docs <- tm_map(docs, stripWhitespace) # Eliminate extra whitespace
      docs <- tm_map(docs, content_transformer(tolower)) # Convert to lower case
      docs <- tm_map(docs, removeWords, stopwords("english")) # Remove stopwords
      docs <- tm_map(docs, removeWords, character(0))
      docs <- tm_map(docs, removePunctuation) # Remove punctuations
      #docs <- tm_map(docs, removeNumbers) # Remove numbers
      #docs <- tm_map(docs, stemDocument) # Stem document
      
      tdm <- TermDocumentMatrix(docs)
      tdm.tidy <- tidy(tdm)
      tdm.var <- summarise(group_by(tdm.tidy, term), sum(count))
      
      pal <- c("Reds","Blues","Greens",
               "Purples","Oranges", "Greys")
      
      wordcloud(tdm.var$term, tdm.var$`sum(count)`,
                scale=c(5,0.5),
                max.words=100,
                min.freq=1,
                random.order=FALSE,
                rot.per=0.3,
                use.r.layout=T,
                random.color=FALSE,
                colors=brewer.pal(5, pal[i])
          )
      title(main = names(In.Group)[i], 
            cex.main=1.5,
            col.main=substr(pal[i],1,nchar(pal[i])-1))
  }
}
```
#### Sentimental Analysis  
```{r}
# Sentence.Length.Sentimetal.Plot
# Create Plot of word count of every sentences in an individual speech
Sentence.Length.Sentimetal.Plot <- function(In.list=sentence.list, In.File, In.Term){
  # Top Emotion Value
  In.list$topemotion.v <- apply(select(In.list, anger:positive), 
                                1, max)
  temp <- In.list$topemotion.v
  In.list$topemotion.v[temp<0.05] <- 1
  
  # Top Emotion Location
  In.list$topemotion <- apply(select(In.list, anger:positive), 
                              1, which.max)
  In.list$topemotion[In.list$topemotion.v<0.05] <- 0
  In.list$topemotion <- In.list$topemotion + 1
 
  # Filter and Select from In.list
  df <- In.list%>%
        filter(File==In.File, 
               Term==In.Term)%>%
        select(sent.id, word.count, 
              topemotion, topemotion.v)
  
  # Set color for the plot
  col.use <- brewer.pal(10,"Set3")
  ptcol.use <- alpha(col.use[df$topemotion], 
                     sqrt(sqrt(sqrt(df$topemotion.v))))
  
  # Plot
  plot(df$sent.id, df$word.count, 
       col=ptcol.use,
       type="h" #,ylim=c(-10, max(In.list$word.count))
       )
  title(main =In.File, xlab = "Sentence ID", ylab = "Number of Words")
}

```

```{r}
# Sentimental.Analysis.Plots 
# Creat Heatmap for correlations and 
# Barplot for average value of emotions for groups in In.Group
Sentimental.Analysis.Plots <- function(In.list=sentence.list, In.Group, In.Group.name){
for (i in 1:length(In.Group)){
#  heatmap.2(cor(In.list%>%
#                filter(File%in%unlist(In.Group[i]))%>%
#                select(anger:trust)), 
#          scale = "none", 
#          col = bluered(100), margin=c(6, 6), key=F,
#          trace = "none", density.info = "none")

  emo.means.var <- colMeans(In.list%>%
                         filter(File%in%unlist(In.Group[i]))%>%
                         select(anger:trust)>0.01)

  barplot(emo.means.var[order(emo.means.var)], 
        las=2, col=brewer.pal(8,"Pastel2")[order(emo.means.var)], 
        horiz=T, main=names(In.Group)[i], xlab = "Average Value of Emotions")
}
}
```

```{r}
# Sentimental.Sentence.Searcher
# Search for sentences that have the max value of each of the 8 emotions for groups in In.Group

Sentimental.Sentence.Searcher <- function(In.list=sentence.list, 
                                    In.Group, 
                                    In.Term=c("1","2","3","4")){
  df <- NULL
  for (i in 1:length(In.Group)){
    var <-   In.list%>%
             filter(File%in%unlist(In.Group[i]), 
                    Term%in%In.Term,
                    word.count>=3)%>%
               select(sentences:trust)
#    var <- as.data.frame(var)
    rst <- as.character(var$sentences[apply(var[,-(1:2)],2,which.max)])
      if (nrow(var)!=0){
        df<- cbind(df,rst) 
      } else {
        df <- cbind(df,c(rep("SYSTEM: No data available in table",8)))
      }
    }
  colnames(df)<- names(In.Group)
  rownames(df) <- c("anger","anticipation","disgust","fear",
                     "joy","sadness","surprise","trust")
  df
}
```

```{r}
# Sentimental.Sentence.Kmeans
# Perform k-means clustering for a group In.group
Sentimental.Sentence.KMeans <- function(In.list=sentence.list, 
                                    In.group, 
                                    In.Term=c("1","2","3","4")){
    var <-   In.list%>%
              filter(File%in%unlist(In.group), 
                     Term%in%In.Term)%>%
              group_by(File)%>%
              summarise(anger=mean(anger),
                        anticipation=mean(anticipation),
                        disgust=mean(disgust),
                        fear=mean(fear),
                        joy=mean(joy),
                        sadness=mean(sadness),
                        surprise=mean(surprise),
                        trust=mean(trust)
                        #negative=mean(negative),
                        #positive=mean(positive)
                        )
    var <- as.data.frame(var)
    for(k in 2:9){
      var[,k][is.na(var[,k])] = 0
    }
    rownames(var) <- as.character(var[,1])
    
    presid.summary <- var
    
    km.res <- kmeans(var[,-1], iter.max=200, 4)
    
    return(list(km.res,presid.summary))
    
}

# Sentimental.Sentence.Cluster
# Visualze clustering results for groups in In.Group
Sentimental.Sentence.Cluster <- function(In.list=sentence.list, 
                                         In.Group, 
                                         In.Term=c("1","2","3","4")){
  for (i in length(In.Group)){
  # Perform k-means clustering 
  clustering.rst <- Sentimental.Sentence.KMeans(In.list,
                                                In.Group[i],
                                                In.Term)
  #Visualze clustering results 
  fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)
  }
  
}  

```



## Analysis  

**Groups of Interest**  
The following factors were considered in this analysis.  

*   [**Overal and Individual Speeches**](#IndivSpeeches)  
*   [**Number of Terms Served**](#TermsServed)  
*   [**Political Party**](#PoliticalParty) 
*   [**Educational Background**](#EducationalBackground) 
*   [**Career Prior to Politics**](#CareerBackground)  
*   [**Served During War Era?**](#WarEra)  

**Types of Analysis**   
For each group of interest, we will perform the following analysis.  

*   Length of Speeches (Number of Words in Speeches)  
*   Length of Sentences 
*   Word Cloud  
*   Sentimental Analysis  
*   Topic Modeling  

### Overal and Individual Speeches  <a name="IndivSpeeches"></a>  {.tabset}  
#### Overview  
The scope of this analysis includes 58 Inaugural Speeches of 39 POTUS, which are scrapped from [The American Presidentcy Project][1]. Among them, there are five inaugural addresses considered the best of all time, which includes Thomas Jefferson's 1st (1801), Abraham Lincoln's 2nd (1865), Franklin Roosevelt's 1st (1933), Franklin Roosevelt's 2nd (1937), and John F. Kennedy's (1961). We will take a closer look at them and see if we can find any interesting facts, similaries or differences, among these five speeches, which may contribute to the great success of them. Also, the most recent four POTUS may sound more familiar than others to our Millennials. So let's take them into consideration as the analysis of individual speecn as well.  
```{r}
print("The Number of Inaugural Speeches in Scope:")
nrow(inaug.list)
print("The Number of POTUS in Scope")
length(unique(inaug.list$File))
print("These POTUS are ")
unique(inaug.list$File)
```

#### Staging  
```{r}
# Read in all the speeches  
folder.path <- "../data/InauguralSpeeches/"
speeches <- list.files(path = folder.path, pattern = "*.txt")
#prex.out <- substr(speeches, 6, nchar(speeches)-4)
docs <- Corpus(DirSource(folder.path))
#docs <- Corpus(VectorSource(inaug.list$Fulltext))

# Clean the text document
docs <- tm_map(docs, stripWhitespace) # Eliminate extra whitespace
docs <- tm_map(docs, content_transformer(tolower)) # Convert to lower case
docs <- tm_map(docs, removeWords, stopwords("english")) # Remove stopwords
docs <- tm_map(docs, removeWords, character(0))
docs <- tm_map(docs, removePunctuation) # Remove punctuations
#docs <- tm_map(docs, removeNumbers) # Remove numbers
#docs <- tm_map(docs, stemDocument) # Stem document

tdm.all <- TermDocumentMatrix(docs)
tdm.tidy <- tidy(tdm.all)
tdm.overall <- summarise(group_by(tdm.tidy, term), sum(count))

# Generate `TF-IDF Weighted Document-Term Matrices`  
dtm.all <- DocumentTermMatrix(docs,
                              control = list(weighting = function(x)
                                                weightTfIdf(x,
                                                            normalize =FALSE),
                                             stopwords = TRUE))
dtm.tidy <- tidy(dtm.all)
#dtm.all <- DocumentTermMatrix(docs)
# Convert rownames to filenames
#rownames(dtm.all) <- paste(corpus.list$File, corpus.list$Term, 
#                           corpus.list$sent.id, sep="_")

#rowTotals <- apply(dtm.all , 1, sum) #Find the sum of words in each Document
#dtm.all  <- dtm.all[rowTotals> 0, ]
#corpus.list <- corpus.list[rowTotals>0, ]
```
Prepare for LDA Topic Modeling
```{r}
# Create corpus list
corpus.list <- sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre <- sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post <- sentence.list$sentences[3:(nrow(sentence.list)-1)]

corpus.list$snipets <- paste(sentence.pre, corpus.list$sentences, 
                             sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]

docs <- Corpus(VectorSource(corpus.list$snipets))
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

# Clean the text document
docs <- tm_map(docs, stripWhitespace) # Eliminate extra whitespace
docs <- tm_map(docs, content_transformer(tolower)) # Convert to lower case
docs <- tm_map(docs, removeWords, stopwords("english")) # Remove stopwords
docs <- tm_map(docs, removeWords, character(0))
docs <- tm_map(docs, removePunctuation) # Remove punctuations
docs <- tm_map(docs, removeNumbers) # Remove numbers
docs <- tm_map(docs, stemDocument) # Stem document

# Generate `TF-IDF Weighted Document-Term Matrices`  
#dtm.all.lda <- DocumentTermMatrix(docs,
#                          control = list(weighting = function(x)
#                                             weightTfIdf(x,
#                                                         normalize =FALSE),
#                                         stopwords = TRUE))
dtm.all.lda <- DocumentTermMatrix(docs)
# Convert rownames to filenames
rownames(dtm.all.lda) <- paste(corpus.list$File, corpus.list$Term, 
                           corpus.list$sent.id, sep="_")
#Find the sum of words in each Document 
rowTotals <- apply(dtm.all.lda , 1, sum) 
dtm.all.lda  <- dtm.all.lda[rowTotals> 0, ]
corpus.list <- corpus.list[rowTotals>0, ]
```

#### Length of Speeches  
```{r, fig.height=5, fig.width=5}
# Generate bar plot of the length of speeches
bxpdat.speech <- boxplot(inaug.list$Words, xlab="Number of Words in a Speech")
text(x=1.2, y=bxpdat.speech$out, 
     labels=inaug.list$President[which(inaug.list$Words%in%bxpdat.speech$out)], 
     cex=0.7, col="blue")
speech.len.file <- inaug.list$File[order(inaug.list$Words, decreasing =TRUE)]
```
Below is a summary of the length of speeches
```{r}
summary(inaug.list$Words)
```
Below listed the top three POTUS who had the longest and shortest speeches respectively.  
The top three POTUS who had the longest speeches are 
```{r}
head(speech.len.file,3)
```
The top three POTUS who had the shortes speeches are
```{r}
tail(speech.len.file,3)
```

#### Length of Sentences 
```{r, fig.height=5, fig.width=5}
bxpdat.sentence <- boxplot(sentence.list$word.count, 
                           xlab="Number of Words in a Sentence")
text(x=1.2, y=bxpdat.sentence$out, 
     labels=inaug.list$President[which(sentence.list$word.count%in%bxpdat.sentence$out)], 
     cex=0.5, col="blue")

# Summary of the length of sentences
print("Below is a summary of the length of sentences")
summary(sentence.list$word.count)
```
Below listed the shortest and longest sentenses in the speech. 
```{r}
sentence.len.file <- sentence.list%>%
                      filter(word.count>=3)%>%
                      arrange(desc(word.count))%>%
                      select(sentences)
tail(sentence.len.file,5)
head(sentence.len.file,5)
```

#### Word Cloud  
**Wordcloud for all speeches**  
The words which appeared more often in speeches are larger in size and darker in color as shown in the words cloud. With no surprise , the most frequent words in all speeches include "will", "government", and "people". 
```{r, fig.height=5, fig.width=5}
set.seed(123)
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(5,"Blues")
          ) 
title(main="What Are The Most Common Words in Inaugural Speeches?", cex.main=0.9)
```
**Interactive Wordcloud for individual speeches**   
This coould be used to find the wordcloud of each individual speech as well as to compare between two different speeches. 
```{r, warning=FALSE}
library(shiny)

shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1', 
                              speeches, selected=speeches[5])),
        column(4, selectInput('speech2', 'Speech 2', 
                              speeches, selected=speeches[9])),
        column(4, sliderInput('nwords', 'Number of words', 3, 
                              min = 20, max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),
    
    server = function(input, output, session) {
      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=dtm.tidy$term[dtm.tidy$document==
                                     as.character(which(speeches == input$speech1))],
             dtm.count1=dtm.tidy$count[dtm.tidy$document==
                                       as.character(which(speeches == input$speech1))],
             dtm.term2=dtm.tidy$term[dtm.tidy$document==
                                     as.character(which(speeches == input$speech2))],
             dtm.count2=dtm.tidy$count[dtm.tidy$document==
                                       as.character(which(speeches == input$speech2))])
      })
      
      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0.3,
              use.r.layout=T,
              random.color=FALSE,
              colors=brewer.pal(5,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0.3,
              use.r.layout=T,
              random.color=FALSE,
              colors=brewer.pal(5,"Blues"), 
            main=input$speech2)
      })
    },

    options = list(height = 600)
)

```
#### Sentimental Analysis  
** indiviudal speeches**  
The most recent four POTUS may sound more familiar than others to our Millennials. Let's take a closer look at how their emotion changed across over the speeches. Below shows the analysis of speeches form both the first and second terms, while to note that President Trump is currently in his first term.   
```{r, fig.width = 5, fig.height = 10, warning=FALSE}
par(mar=c(2,2,2,1), mfrow=c(7,1))
Sentence.Length.Sentimetal.Plot(sentence.list,"WilliamJClinton",1)
Sentence.Length.Sentimetal.Plot(sentence.list,"WilliamJClinton",2)
Sentence.Length.Sentimetal.Plot(sentence.list,"GeorgeWBush",1)
Sentence.Length.Sentimetal.Plot(sentence.list,"GeorgeWBush",2)
Sentence.Length.Sentimetal.Plot(sentence.list,"BarackObama",1)
Sentence.Length.Sentimetal.Plot(sentence.list,"BarackObama",2)
Sentence.Length.Sentimetal.Plot(sentence.list,"DonaldJTrump",1)
```
Also, there are five inaugural addresses considered the best across all time. It includes Thomas Jefferson's 1st(1801), Abraham Lincoln's 2nd(1865), Franklin Roosevelt's 1st(1933), Franklin Roosevelt's 2nd(1937), John F. Kennedy's(1961). Let's take a closer look at them and see if you can find any similaries and differences among these five speeches, which may contribute to the great success of them. 
```{r, fig.width = 5, fig.height = 10, warning=FALSE}
par(mar=c(2,2,2,1), mfrow=c(7,1))
Sentence.Length.Sentimetal.Plot(sentence.list,"ThomasJefferson",1)
Sentence.Length.Sentimetal.Plot(sentence.list,"AbrahamLincoln",2)
Sentence.Length.Sentimetal.Plot(sentence.list,"FranklinDRoosevelt",1)
Sentence.Length.Sentimetal.Plot(sentence.list,"FranklinDRoosevelt",2)
Sentence.Length.Sentimetal.Plot(sentence.list,"JohnFKennedy",1)
```

#### Topic Modeling 
```{r}
# Run LDA (Latent Dirichlet allocation)

# Set parameters for Gibbs sampling
burnin <- 4000 # Drop the first 4000 samples
iter <- 2000
thin <- 500 # Pick only every 500 guesses
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE # Sample with the largest posterior distribution 
k <- 8 # Number of topics

# Run LDA using Gibbs sampling
ldaOut.all <-LDA(dtm.all.lda, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
# Write out results
# Docs to topics
ldaOut.topics.all <- as.matrix(topics(ldaOut.all))
table(c(1:k, ldaOut.topics.all))
write.csv(ldaOut.topics.all,file=paste("../output/LDAGibbs ",
                                   k," DocsToTopics.csv", sep = ''))

# Top 10 terms in each topic
ldaOut.terms.all <- as.matrix(terms(ldaOut.all,10))
write.csv(ldaOut.terms.all,file=paste("../output/LDAGibbs ",
                                  k," TopicsToTerms.csv", sep = ''))

# Probabilities associated with each topic assignment
topicProbabilities.all <- as.data.frame(ldaOut.all@gamma)
write.csv(topicProbabilities.all,file=paste("../output/LDAGibbs ",
                                        k," TopicProbabilities.csv", sep = ''))

terms.beta <- ldaOut.all@beta
terms.beta <- scale(terms.beta)
topics.terms <- NULL
for(i in 1:k){
  topics.terms.all <- rbind(topics.terms, 
                        ldaOut.all@terms[order(terms.beta[i,], 
                                           decreasing = TRUE)[1:10]])
}
#topics.terms.all
ldaOut.terms.all[1:3,]
```

Based on the most popular terms and the most salient terms for each topic, we assign a hashtag to each topic.
```{r}
topics.hash <- c("Goverment", "War", "World", "Law", 
                 "Time", "Nation", "People", "Country")
print(topics.hash)
corpus.list$ldatopic <- as.vector(ldaOut.topics.all)
corpus.list$ldahash <- topics.hash[ldaOut.topics.all]

colnames(topicProbabilities.all) <- topics.hash
corpus.list.df <- cbind(corpus.list, topicProbabilities.all)
```
We can use Word Cloud to visualize  the top topics as shown below. 
```{r, fig.height=5, fig.width=4.5}
set.seed(123)
wordcloud(corpus.list.df$ldahash,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(5,"Blues")
          )
```
### Number of Terms Served  <a name="TermsServed"></a>  {.tabset}  
#### Overview  
**Summary of Group of Interest**

| Number of Terms Served | Number of Presidents |
|:----------------------:|:--------------------:|     
| One Term               | 22                   | 
| Two Terms              | 16                   |  
| Four Terms             |  1                   | 

This analysis has primarily focused on presidents who served one or two terms. The analysis of the speeches of Franklin Roosevelt, who was the only POTUS who served more than two terms, may be incorporated in the furture. 

#### Staging  
```{r}
### Number of Terms Served
# Summary of groups by the number of terms served
inaug.list%>%
  group_by(Term)%>%
  summarise(Count = n_distinct(President))

# Identify 'File' for each group 
twoplus.terms <- inaug.list$File[inaug.list$Term==2]
four.terms <- unique(inaug.list$File[inaug.list$Term>2])
two.terms <- twoplus.terms[twoplus.terms != four.terms]
one.term <- inaug.list$File[!(inaug.list$File%in%twoplus.terms)]

# Create group and group name
TermGroup <- list("One Term"=one.term, "Two Terms"=two.terms, "Four Terms"=four.terms)
TermGroup.name <- "Number of Terms Served"
```
POTUS are assigned into 3 groups by the number of terms they served. 

*   22 POTUS who have served only one term:
```{r}
TermGroup$`One Term`
```
*   16 POTUS who have served two terms: 
```{r}
TermGroup$`Two Terms`
```
*   One POTUS who has served more than two terms: 
```{r}
TermGroup$`Four Terms`
```

#### Length of Speeches  
```{r, fig.width = 15, fig.height = 5, warning=FALSE}
par(mar=c(4.5,3,2,3), mfrow=c(1,2))

# Create bar plot of each speech
Speech.Length.Bar.Plot(inaug.list,TermGroup,TermGroup.name)
# Create box plot of each speech by group 
Speech.Length.Box.Plot(inaug.list,TermGroup,TermGroup.name)
```
For POTUS who served two terms, we would like to see if there is any differece between their first term and second term.  
```{r, fig.width = 15, fig.height = 5, warning=FALSE}
par(mar=c(4.5,3,2,3), mfrow=c(1,2))

# Create bar plot and box plot for 'Two Terms' group
i.var <- inaug.list%>%
          filter(File%in%unlist(TermGroup[2]),
                 Term%in%c(1,2))
i.var$File <- factor(i.var$File)
i.var$FileOrdered <- reorder(i.var$File, i.var$Words, mean, order=T)
  
i.x <- barplot(i.var$Words, space = 1,
               col = brewer.pal(9,"Blues")[c(4,7)],
               cex.axis = 0.7,
               main = "Number of Words in a Speech" 
              )
text(cex=0.5, x=i.x-0.25, y=-1.25, 
       adj=1, srt = 60, xpd = TRUE,
       labels = paste(substr(i.var$Date,1,4),
                      i.var$President))
legend("topright", inset= 0.02, legend =c("First Term", "Second Term"), 
         text.col = brewer.pal(9,"Blues")[c(4,7)], 
         cex = 0.7, box.lty=2 )

  boxplot(i.var$Words~i.var$Term,
          col=alpha(brewer.pal(9,"Blues"),1)[c(4,7)],
          cex.axis = 0.7,
          main = "Number of Words in a Speech")

```

#### Length of Sentences 
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,2))

Bee.Swarm.Plot(sentence.list,TermGroup,TermGroup.name)
Bee.Swarm.Plot.Group(sentence.list,TermGroup,TermGroup.name)
```
Below listed the shortest and longest sentenses in the speech. 
```{r, warning=FALSE}
Sentence.Searcher.Group(sentence.list,TermGroup,c("1","2","3","4"),FALSE)
Sentence.Searcher.Group(sentence.list,TermGroup,c("1","2","3","4"),TRUE)
```
For POTUS who served two terms, we would like to see if there is any differece between the their first term and sencond term.  
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,2))
# Create bee swarm plot for 'Two Terms' group

ii.var <- sentence.list%>%
            filter(File%in%unlist(TermGroup[2]),
                   Term%in%c(1,2))
ii.var$File <- factor(ii.var$File)
ii.var$FileOrdered <- reorder(ii.var$File, ii.var$word.count, mean, order=T)
  
beeswarm(word.count~FileOrdered, 
             data = ii.var%>%filter(Term==1),
             horizontal = TRUE,
             pch=16, col=alpha(brewer.pal(9, "Blues")[4], 1), 
             cex=0.55, cex.axis=0.55, cex.lab=1,
             spacing=5/nlevels(ii.var$FileOrdered),
             las=2, ylab="", xlab="",
             main="Number of Words in a Sentence")
beeswarm(word.count~FileOrdered, 
             data = ii.var%>%filter(Term==2),
             add=TRUE,
             horizontal = TRUE,
             pch=16, col=alpha(brewer.pal(9, "Blues")[7], 1), 
             cex=0.55, cex.axis=0.55, cex.lab=1,
             spacing=5/nlevels(ii.var$FileOrdered))

legend("bottomright", inset= 0.02, legend =c("First Term", "Second Term"), 
         text.col = brewer.pal(9,"Blues")[c(4,7)], 
         cex = 0.7, box.lty=2 )

beeswarm(word.count~Term, 
             data = ii.var,
             horizontal = TRUE,
             pch=16, col=alpha(brewer.pal(9,
                                          "Blues")[c(4,7)], 
                               0.4), 
             cex=0.55, cex.axis=0.55, cex.lab=1,
             spacing=5/nlevels(ii.var$FileOrdered),
             las=2, ylab="", xlab="",
             main="Number of Words in a Sentence")
  legend("bottomright", inset=0.02, legend =c("First Term", "Second Term") , 
         text.col = brewer.pal(9,"Blues")[c(4,7)], 
         cex = 0.7, box.lty=2 )
  boxplot(ii.var$word.count~ii.var$Term, horizontal=TRUE, 
          col="#0000ff22", axes=FALSE, add=TRUE)
```

Below listed the shortest and longest sentenses in the speeches of presidents who served two terms. 
```{r, warning=FALSE}
Sentence.Searcher.Group(sentence.list,TermGroup,c("1"),FALSE)[,2]
Sentence.Searcher.Group(sentence.list,TermGroup,c("2"),FALSE)[,2]

Sentence.Searcher.Group(sentence.list,TermGroup,c("1"),TRUE)[,2]
Sentence.Searcher.Group(sentence.list,TermGroup,c("2"),TRUE)[,2]
```

#### Word Cloud  
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,length(TermGroup)))
set.seed(123)
Word.Cloud.Group(inaug.list,TermGroup,TermGroup.name)
```
For POTUS who served two terms, we would like to see if there is any differece between the their first term and sencond term. 
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,2))
set.seed(123)
for (i in 1:2){
  iii.var <- inaug.list%>%
             filter(File%in%unlist(TermGroup[2]))

  iii.docs <- Corpus(VectorSource(iii.var$Fulltext))
  iii.docs <- tm_map(iii.docs, stripWhitespace) # Eliminate extra whitespace
  iii.docs <- tm_map(iii.docs, content_transformer(tolower)) # Convert to lower case
  iii.docs <- tm_map(iii.docs, removeWords, stopwords("english")) # Remove stopwords
  iii.docs <- tm_map(iii.docs, removeWords, character(0))
  iii.docs <- tm_map(iii.docs, removePunctuation) # Remove punctuations
  #iii.docs <- tm_map(iii.docs, removeNumbers) # Remove numbers
  #iii.docs <- tm_map(iii.docs, stemDocument) # Stem document
      
  iii.tdm <- TermDocumentMatrix(iii.docs)
  iii.tdm.tidy <- tidy(iii.tdm)
  iii.tdm.var <- summarise(group_by(iii.tdm.tidy, term), sum(count))
      

  wordcloud(iii.tdm.var$term, iii.tdm.var$`sum(count)`,
                scale=c(5,0.5),
                max.words=100,
                min.freq=1,
                random.order=FALSE,
                rot.per=0.3,
                use.r.layout=T,
                random.color=FALSE,
                colors=brewer.pal(5, "Blues")
          )
  title(main = c("First Term","Second Term")[i], 
            cex.main=1.5,
            col.main=brewer.pal(5, "Blues")[5])
}
```

#### Sentimental Analysis  
**Clustering of Emotions** 
```{r, fig.width = 5, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(length(TermGroup),1))
# Create barplot of average value of emotions for each group
Sentimental.Analysis.Plots(sentence.list,TermGroup,TermGroup.name)
```
```{r, fig.width = 5, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(2,1))
# Creat Heatmap for correlations and 
# Barplot for average value of emotions for groups in In.Group

#  heatmap.2(cor(sentence.list%>%
#                filter(File%in%unlist(TermGroup[2]))%>%
#                select(anger:trust)), 
#          scale = "none", 
#          col = bluered(100), margin=c(6, 6), key=F,
#          trace = "none", density.info = "none")

  i.emo.means.var <- colMeans(sentence.list%>%
                         filter(File%in%unlist(TermGroup[2]),Term==1)%>%
                         select(anger:trust)>0.01)
  ii.emo.means.var <- colMeans(sentence.list%>%
                         filter(File%in%unlist(TermGroup[2]),Term==2)%>%
                         select(anger:trust)>0.01)
  barplot(i.emo.means.var[order(i.emo.means.var)], 
        las=2, col=brewer.pal(8,"Pastel2")[order(i.emo.means.var)], 
        horiz=T, main="First Term", xlab = "Average Value of Emotions")
  barplot(ii.emo.means.var[order(ii.emo.means.var)], 
        las=2, col=brewer.pal(8,"Pastel2")[order(ii.emo.means.var)], 
        horiz=T, main="Second Term", xlab = "Average Value of Emotions")

```
As shown, the fear level increases but the sadness level decreases from first term to second term, while all other emotions seemed at the same level as the first term. We would like to look into it closer and try to indentify the most influced sentences that drove the changes.    
```{r}
i.emo.var <- sentence.list%>%
                         filter(File%in%unlist(TermGroup[2]),Term==1)%>%
                         select(sentences, fear,sadness)
ii.emo.var <- sentence.list%>%
                         filter(File%in%unlist(TermGroup[2]),Term==2)%>%
                         select(sentences, fear,sadness)

i.fear <- head(i.emo.var%>%arrange(desc(fear))%>%select(sentences))
ii.fear <- head(ii.emo.var%>%arrange(desc(fear))%>%select(sentences))
i.sadness <- head(i.emo.var%>%arrange(desc(sadness))%>%select(sentences))
ii.dadness <- head(ii.emo.var%>%arrange(desc(sadness))%>%select(sentences))
i.emo.df <- cbind(i.fear,ii.fear,i.sadness,ii.dadness)
colnames(i.emo.df) <- c("First Term Fear","Second Term Fear",
                        "Fist Term Sadness", "Second Term Sadness")
i.emo.df
```

**What are the emotionally charged sentences?** 
```{r}
# What are the emotionally charged sentences?  
Sentimental.Sentence.Searcher(sentence.list,TermGroup,c("1","2","3","4"))

Sentimental.Sentence.Searcher(sentence.list,TermGroup[2],c("1"))
Sentimental.Sentence.Searcher(sentence.list,TermGroup[2],c("2"))
```

```{r, fig.width = 10, fig.height = 6, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(2,1))
# One Term
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,TermGroup[1],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)

# Two  Terms
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,TermGroup[2],c("1","2","3","4"))

## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)
```
For POTUS who served two terms, we would like to see if there is any differece between their first term and second term.  
```{r, fig.width = 10, fig.height = 6, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(2,1))
# One Term
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,TermGroup[2],c("1"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)

# Two  Terms
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,TermGroup[2],c("2"))

## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

#### Topic Modeling
  

### Political Party  <a name="PoliticalParty"></a>  {.tabset}
#### Overview  
| Political Party | Number of Presidents |
|:---------------:|:--------------------:|     
| Republican      | 17                   | 
| Democratic      | 14                   |  

#### Staging 
```{r}
### Political Party
# Summary of groups by politial party
inaug.list%>%
  group_by(Party)%>%
  summarise(Count = n_distinct(President))

# Identify 'File' for each group 
Democrats <- inaug.list$File[inaug.list$Party=="Democratic"]
Republicans <- inaug.list$File[inaug.list$Party=="Republican"]

# Create group and group name
PartyGroup <- list("Republicans"=Republicans, "Democrats"=Democrats)
PartyGroup.name <- "Political Party"
```
31 POTUS are assigned into 2 groups by the political party they served, i.e., Republican vs. Democratic. The rest of the POTUS will not be in scope of the analysis in terms of the political party they served.   

*   17 POTUS who are Republicans:  
```{r}
PartyGroup$Republicans
```

*   14 POTUS who are Democrats:  
```{r}
PartyGroup$Democrats
```
#### Length of Speeches  
```{r, fig.width = 15, fig.height = 5, warning=FALSE}
par(mar=c(4.5,3,2,3), mfrow=c(1,2))

# Create bar plot of each speech
Speech.Length.Bar.Plot(inaug.list,PartyGroup,PartyGroup.name)
# Create box plot of each speech by group 
Speech.Length.Box.Plot(inaug.list,PartyGroup,PartyGroup.name)
```
#### Length of Sentences   
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,2))

Bee.Swarm.Plot(sentence.list,PartyGroup,PartyGroup.name)
Bee.Swarm.Plot.Group(sentence.list,PartyGroup,PartyGroup.name)
```
Below listed the shortest and longest sentenses in the speech. 
```{r, warning=FALSE}
Sentence.Searcher.Group(sentence.list,PartyGroup,c("1","2","3","4"),FALSE)
Sentence.Searcher.Group(sentence.list,PartyGroup,c("1","2","3","4"),TRUE)

```

#### Word Cloud  
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,length(PartyGroup)))
set.seed(123)

Word.Cloud.Group(inaug.list,PartyGroup,PartyGroup.name)
```
#### Sentimental Analysis  
**Clustering of Emotions** 
```{r, fig.width = 5, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(length(PartyGroup),1))
# Create barplot of average value of emotions for each group
Sentimental.Analysis.Plots(sentence.list,PartyGroup,PartyGroup.name)
```
As shown, the fear level of speeches of Democrats is higher than that of Republicans, while all other emotions seemed to be at the same level as them. We would like to look into it closer and try to indentify the most influced sentences that drove the difference.    
```{r}
i.emo.var <- sentence.list%>%
                         filter(File%in%unlist(PartyGroup[1]))%>%
                         select(sentences, fear)
ii.emo.var <- sentence.list%>%
                         filter(File%in%unlist(PartyGroup[2]))%>%
                         select(sentences, fear)

i.fear <- head(i.emo.var%>%arrange(desc(fear))%>%select(sentences))
ii.fear <- head(ii.emo.var%>%arrange(desc(fear))%>%select(sentences))

i.emo.df <- cbind(i.fear,ii.fear)
colnames(i.emo.df) <- c("Republicans Fear","Democrats Fear")
i.emo.df
```

**What are the emotionally charged sentences?** 
```{r}
# What are the emotionally charged sentences?  
Sentimental.Sentence.Searcher(sentence.list,PartyGroup,c("1","2","3","4"))
```

```{r, fig.width = 10, fig.height = 5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(2,1))
# Reputlican
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,PartyGroup[1],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)

# Democratic
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,PartyGroup[2],c("1","2","3","4"))

## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

### Educational Background <a name="EducationalBackground"></a>  {.tabset}   
#### Overview  

| Educational Background | Number of Presidents |
|:----------------------:|:--------------------:|     
| No Colledge            | 9                    | 
| Undergraduate          | 21                   |  
| Advanced Degree        | 7                    | 

#### Staging  
Educational backgound of POTUS were obtained from the following websites: 

* [List of POTUS by education][3]   
* [Educational and Career Backgrounds of POTUS][4]  

```{r}
### Educational Background
# Indentify educational background of each POTUS 
NoColledge <- c("GeorgeWashington", "JamesMonroe", "AndrewJackson",
                "MartinvanBuren", "ZacharyTaylor", "AbrahamLincoln",
                "GroverCleveland", "WilliamMcKinley", "HarrySTruman")
Undergrad <- c("ThomasJefferson","JamesMadison", "JohnQuincyAdams",
               "JamesKPolk", "FranklinPierce", "JamesBuchanan", 
               "UlyssesSGrant", "JamesGarfield", "BenjaminHarrison", 
               "TheodoreRoosevelt", "WarrenGHarding", "CalvinCoolidge", 
               "HerbertHoover", "FranklinDRoosevelt", "DwightDEisenhower", 
               "JohnFKennedy", "LyndonBJohnson", "JimmyCarter", 
               "RonaldReagan", "GeorgeBush", "DonaldJTrump")
MA.MS <- c("JohnAdams")
MBA <- c("GeorgeWBush")
Law <- c("RutherfordBHayes", "RichardNixon", "WilliamJClinton", "BarackObama")
Doctorate <- c("WoodrowWilson")

Advanced.Degree <- c(MA.MS, MBA, Law, Doctorate) 

# Group POTUS into three categories and Create group and group name
EducationGroup <- list("No Colledge"=NoColledge,
                        "Undergraduate"=Undergrad,
                        "Advanced Degree"=Advanced.Degree)
EducationGroup.name <- "Educational Background"

summary(Education.Group)[,1]
```
POTUS are assigned into 3 groups based on their highest education level. 

*   9 POTUS without a colledge degree include: 
```{r}
EducationGroup$`No Colledge`
```
*  21 POTUS with only undergraduate degree include: 
```{r}
EducationGroup$Undergraduate
```
*  7 POTUS with more advanced degree include: 
```{r}
EducationGroup$`Advanced Degree`
```

#### Length of Speeches  
```{r, fig.width = 15, fig.height = 5, warning=FALSE}
par(mar=c(4.5,3,2,3), mfrow=c(1,2))

# Create bar plot of each speech
Speech.Length.Bar.Plot(inaug.list,EducationGroup,EducationGroup.name)
# Create box plot of each speech by group 
Speech.Length.Box.Plot(inaug.list,EducationGroup,EducationGroup.name)
```
#### Length of Sentences 
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,2))

Bee.Swarm.Plot(sentence.list,EducationGroup,EducationGroup.name)
Bee.Swarm.Plot.Group(sentence.list,EducationGroup,EducationGroup.name)
```
Below listed the shortest and longest sentenses in the speech. 
```{r, warning=FALSE}
Sentence.Searcher.Group(sentence.list,EducationGroup,c("1","2","3","4"),FALSE)
Sentence.Searcher.Group(sentence.list,EducationGroup,c("1","2","3","4"),TRUE)
```
#### Word Cloud  
```{r, fig.width = 15, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,length(EducationGroup)))
set.seed(123)

Word.Cloud.Group(inaug.list,EducationGroup,EducationGroup.name)
```

#### Sentimental Analysis  
**Clustering of Emotions** 
```{r, fig.width = 5, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(length(EducationGroup),1))
# Create barplot of average value of emotions for each group
Sentimental.Analysis.Plots(sentence.list,EducationGroup,EducationGroup.name)
```

**What are the emotionally charged sentences?** 
```{r}
# What are the emotionally charged sentences?  
Sentimental.Sentence.Searcher(sentence.list,EducationGroup,c("1","2","3","4"))
```

```{r, fig.width = 15, fig.height = 10, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(2,1))
# No Colledge Degree 
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,EducationGroup[1],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)

# Undergraduate Degree Only
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,EducationGroup[2],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)

# Advenced Degree
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,EducationGroup[3],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)

```

### Career Prior to Politics  <a name="CareerBackground"></a>  {.tabset}
#### Overview  

| Career Prior to Politics | Number of Presidents |
|:------------------------:|:--------------------:|     
| Lawyer                   | 21                   | 
| Military Leader          | 3                    |  
| Farmer                   | 3                    | 
| Educator                 | 2                    |  
| Businessperson           | 3                    | 

#### Staging  
Career backgound of POTUS were obtained from the following website: 

* [Educational and Career Backgrounds of POTUS][4]  

```{r}
### Career Prior to Politics  
# Indentify career background of each POTUS 
lawyer <- c("JohnAdams", "ThomasJefferson", "JamesMadison", 
            "JamesMonroe", "JohnQuincyAdams", "AndrewJackson", 
            "MartinvanBuren", "JamesKPolk", "FranklinPierce", 
            "JamesBuchanan", "AbrahamLincoln", "RutherfordBHayes", 
            "JamesGarfield", "GroverCleveland", "BenjaminHarrison", 
            "WilliamMcKinley", "CalvinCoolidge", "FranklinDRoosevelt", 
            "RichardNixon", "WilliamJClinton", "BarackObama")
military.leader <- c("ZacharyTaylor", "UlyssesSGrant", "DwightDEisenhower")
farmer <- c("GeorgeWashington", "HarrySTruman", "JimmyCarter")
businessperson <- c("GeorgeBush", "GeorgeWBush", "DonaldJTrump")
educator <- c("WoodrowWilson", "LyndonBJohnson")

# Group POTUS into five categories and Create group and group name
CareerGroup <- list("Lawyer"=lawyer, 
               "Military Learder"=military.leader,
               "Farmer"=farmer,
               "Educator"=educator, 
               "Businessperson"=businessperson)
CareerGroup.name <- "Career Prior to Politics"
summary(Career.Group)[,1]
```

POTUS are assigned into 5 groups based on their career pior to politics. 

*   21 POTUS were lawywers include: 
```{r}
CareerGroup$Lawyer
```
*  3 POTUS were military leaders: 
```{r}
CareerGroup$`Military Learder`
```
*  3 POTUS were farmers: 
```{r}
CareerGroup$Farmer
```
*  2 POTUS were educators: 
```{r}
CareerGroup$Educator
```
*  3 POTUS were businesspersons: 
```{r}
CareerGroup$Businessperson
```

#### Length of Speeches  
```{r, fig.width = 15, fig.height = 5, warning=FALSE}
par(mar=c(4.5,3,2,3), mfrow=c(1,2))

# Create bar plot of each speech
Speech.Length.Bar.Plot(inaug.list,CareerGroup,CareerGroup.name)
# Create box plot of each speech by group 
Speech.Length.Box.Plot(inaug.list,CareerGroup,CareerGroup.name)
```
#### Length of Sentences 
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,2))

Bee.Swarm.Plot(sentence.list,CareerGroup,CareerGroup.name)
Bee.Swarm.Plot.Group(sentence.list,CareerGroup,CareerGroup.name)
```
Below listed the shortest and longest sentenses in the speech. 
```{r, warning=FALSE}
Sentence.Searcher.Group(sentence.list,CareerGroup,c("1","2","3","4"),FALSE)
Sentence.Searcher.Group(sentence.list,CareerGroup,c("1","2","3","4"),TRUE)
```
#### Word Cloud  
```{r, fig.width = 15, fig.height = 6, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,length(CareerGroup)))
set.seed(123)

Word.Cloud.Group(inaug.list,CareerGroup,CareerGroup.name)
```

#### Sentimental Analysis  
**Clustering of Emotions** 
```{r, fig.width = 5, fig.height = 10, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(length(CareerGroup),1))
# Create barplot of average value of emotions for each group
Sentimental.Analysis.Plots(sentence.list,CareerGroup,CareerGroup.name)
```

**What are the emotionally charged sentences?** 
```{r}
# What are the emotionally charged sentences?  
Sentimental.Sentence.Searcher(sentence.list,CareerGroup,c("1","2","3","4"))
```
We would like to take a look at the cluster plot of the Lawyer group, as size of other groups are quite small. 
```{r, fig.width = 10, fig.height = 5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(2,1))
# Lawyer
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,CareerGroup[1],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

### Served During War Era?  <a name="WarEra"></a>  {.tabset}     
#### Overview  

| Served During War Era? | Number of Presidents |
|:----------------------:|:--------------------:|     
| No War Era             | 38                   | 
| War Era                | 11                   |  

#### Staging  
The following website was used as a reference to understand more about the U.S. history. It presents a timeline of the U.S. hitory, which specifies the wars the U.S. was involded in. 

* [Animated Atlas][5]

POTUS are assigned into two groups based on whether it was war era while they were serving. 

*   11 POTUS who served during war eras include: 
James Madison(War of 1812), James Polk(Mexican War), Abraham Lincoln(Civil War), William McKinley(Spanish-American War), Woodrow Wilson(WWI), Franklin D. Roosevelt(WWII), Harry S. Truman(Korean War), Lyndon Johnson(Vietam War), Richard Nixon(Vietnam War), George Bush(Gulf War), George W. Bush(War on Terror).  

*   The rest 38 POTUS were serving not during war era.

```{r}
### Served During War Era?  
# Indentify POTUS who served during war eras 
war.era <- c("JamesMadison", "JamesKPolk", "AbrahamLincoln", 
             "WilliamMcKinley", "WoodrowWilson", "FranklinDRoosevelt",
             "HarrySTruman", "LyndonBJohnson", "RichardNixon",
             "GeorgeBush", "GeorgeWBush")
nowar.era <- inaug.list$File[!(inaug.list$File%in%war.era)] 

# Create group and group name
WarGroup <- list("No War Era"=nowar.era, "War Era"=war.era)
WarGroup.name <- "Whether Served During War Era" 

summary(WarGroup)[,1]
```

#### Length of Speeches  
```{r, fig.width = 15, fig.height = 5, warning=FALSE}
par(mar=c(4.5,3,2,3), mfrow=c(1,2))

# Create bar plot of each speech
Speech.Length.Bar.Plot(inaug.list,WarGroup,WarGroup.name)
# Create box plot of each speech by group 
Speech.Length.Box.Plot(inaug.list,WarGroup,WarGroup.name)
```
#### Length of Sentences 
```{r, fig.width = 10, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,2))

Bee.Swarm.Plot(sentence.list,WarGroup,WarGroup.name)
Bee.Swarm.Plot.Group(sentence.list,WarGroup,WarGroup.name)
```
Below listed the shortest and longest sentenses in the speech. 
```{r, warning=FALSE}
Sentence.Searcher.Group(sentence.list,WarGroup,c("1","2","3","4"),FALSE)
Sentence.Searcher.Group(sentence.list,WarGroup,c("1","2","3","4"),TRUE)
```
#### Word Cloud  
```{r, fig.width = 8, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(1,length(WarGroup)))
set.seed(123)

Word.Cloud.Group(inaug.list,WarGroup,WarGroup.name)
```

#### Sentimental Analysis  
**Clustering of Emotions** 
```{r, fig.width = 5, fig.height = 4.5, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(length(WarGroup),1))
# Create barplot of average value of emotions for each group
Sentimental.Analysis.Plots(sentence.list,WarGroup,WarGroup.name)
```

**What are the emotionally charged sentences?** 
```{r}
# What are the emotionally charged sentences?  
Sentimental.Sentence.Searcher(sentence.list,WarGroup,c("1","2","3","4"))
```
```{r, fig.width = 15, fig.height = 10, warning=FALSE}
par(mar=c(4.5,5.5,2,1), mfrow=c(2,1))
# No War Era
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,WarGroup[1],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)
# War Era
## Perform k-means clustering 
clustering.rst <- Sentimental.Sentence.KMeans(sentence.list,WarGroup[2],c("1","2","3","4"))
## Visualze clustering results 
fviz_cluster(clustering.rst[[1]], 
             stand=F, repel= TRUE,
             data = clustering.rst[[2]][,-1], 
             xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

## References  

* [The American Presidentcy Project][1]
* [List of POTUS][2]   
* [List of POTUS by education][3]   
* [Educational and Career Backgrounds of POTUS][4]  
* [Animated Atlas][5]
* [ADS Github][6]

[1]: http://www.presidency.ucsb.edu/inaugurals.php
[2]: https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States
[3]: https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States_by_education
[4]: https://ijr.com/2015/07/366694-charts-show-educational-work-backgrounds-presidential-candidates/ 
[5]: http://www.animatedatlas.com/timeline.html
[6]:https://github.com/TZstatsADS/ADS_Teaching/tree/master/Tutorials/wk2-TextMining


